{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95caa5f-4aae-4f10-a9b1-ae3abe9a4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c353c-0734-470e-b48d-c243ef8b3ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \"\"\"Simple neural network accepting two features as input and returning a single output\n",
    "\n",
    "    In the context of PINNs, the neural network is used as universal function approximator\n",
    "    to approximate the solution of the differential equation\n",
    "    \"\"\"\n",
    "    def __init__(self, num_hidden: int, dim_hidden: int, dim_input : int = 3, dim_output : int = 2, act=nn.Tanh()):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.layer_in = nn.Linear(dim_input, self.dim_hidden)\n",
    "        nn.init.xavier_uniform_(self.layer_in.weight)\n",
    "\n",
    "        self.num_middle = num_hidden - 1\n",
    "        self.middle_layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(self.num_middle):\n",
    "            middle_layer = nn.Linear(dim_hidden, dim_hidden)\n",
    "            nn.init.xavier_uniform_(middle_layer.weight)\n",
    "            self.act = act\n",
    "            self.middle_layers.append(middle_layer)\n",
    "\n",
    "        self.layer_out = nn.Linear(dim_hidden, dim_output)\n",
    "        nn.init.xavier_uniform_(self.layer_out.weight)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        x_stack = torch.cat([x, t], dim=1)\n",
    "        out = self.act(self.layer_in(x_stack))\n",
    "        for layer in self.middle_layers:\n",
    "            out = self.act(layer(out))\n",
    "        logits = self.layer_out(out)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad0de78-d16d-4a2e-9ea6-36aed8436b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: torch.tensor,\n",
    "        T: torch.tensor,\n",
    "        y_true : torch.tensor,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        self.X = X\n",
    "        self.T = T\n",
    "        self.y_true = y_true \n",
    "        \n",
    "    def loss(self, nn: NN):\n",
    "        output = f(nn, self.X, self.T)\n",
    "        loss = output - self.y_true\n",
    "        return loss.pow(2).mean()\n",
    "\n",
    "    def __call__(self, nn: NN):\n",
    "        \"\"\"\n",
    "        Allows you to use instance of this class as if it was a function:\n",
    "\n",
    "        ```\n",
    "            >>> loss = Loss(*some_args)\n",
    "            >>> calculated_loss = loss(nn)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return self.loss(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125ecb8-3472-4597-9166-70031b3bce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pytz\n",
    "from typing import Callable\n",
    "\n",
    "def train_model(\n",
    "    nn_approximator: NN,\n",
    "    loss_fn: Callable,\n",
    "    learning_rate: int,\n",
    "    max_epochs: int\n",
    ") -> NN:\n",
    "\n",
    "    optimizer = torch.optim.Adam(nn_approximator.parameters(), lr=learning_rate)\n",
    "    loss_values = []\n",
    "    loss: torch.Tensor = torch.inf\n",
    "\n",
    "    pbar = tqdm(total=max_epochs, desc=\"Training\", position=0)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        loss: torch.Tensor = loss_fn(nn_approximator)\n",
    "        loss = loss_fn(nn_approximator)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        # Log loss\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.update(1)\n",
    "    pbar.close()\n",
    "    return nn_approximator, np.array(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca032c48-54e0-4a73-9833-4eebbdcc9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(nn: NN, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute the value of the approximate solution from the NN model\n",
    "    Internally calling the forward method when calling the class as a function\"\"\"\n",
    "    return nn(x, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
