{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pinn import *\n",
    "from nn import NN\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def serialize_savelist(keys: list, values : list, filename):\n",
    "    dict = {k: v for k, v in zip(keys, values)}\n",
    "    filename = f\"{filename}.json\"\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yYakGVopXcjP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters model\n",
    "rho = 0.5\n",
    "mu = 79300 # MPa\n",
    "lam = 100000 # MPa\n",
    "\n",
    "Lx = 0.2\n",
    "Ly = 0.2\n",
    "T = 0.2\n",
    "\n",
    "x_domain = np.array([0.0, Lx])/Lx\n",
    "y_domain = np.array([0.0, Ly])/Lx\n",
    "t_domain = np.array([0.0, T])/T\n",
    "\n",
    "# Parameters NN\n",
    "epochs = 50000\n",
    "n_train = 30\n",
    "layers = 3\n",
    "dim_hidden = 50\n",
    "lr = 0.002\n",
    "om_INITIAL = 1\n",
    "om_BOUNDARY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eventually save changed $NN_2$ parameters to pass to other script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "par = [Lx, T, n_train, layers, dim_hidden]\n",
    "keys = ['x', 't', 'n', 'n_layers', 'n_neurons']\n",
    "\n",
    "serialize_savelist(keys, par, 'par')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinn = PINN(layers, dim_hidden, act=nn.Tanh()).to(device)\n",
    "\n",
    "loss_fn = Loss(\n",
    "    x_domain,\n",
    "    y_domain,\n",
    "    t_domain,\n",
    "    n_train,\n",
    "    return_adim(x_domain, t_domain, rho, mu, lam),\n",
    "    initial_conditions,\n",
    "    om_INITIAL,\n",
    "    om_BOUNDARY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import weights from pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_model = get_last_modified_file('in_model')\n",
    "pretrained_model_dict = torch.load(filename_model, map_location=torch.device(device))\n",
    "\n",
    "pretrained_model = NN(layers, dim_hidden, 2, 1)\n",
    "pretrained_model.load_state_dict(pretrained_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in np.arange(len(pinn.middle_layers)):\n",
    "    pinn_layer = pinn.middle_layers[i]\n",
    "    pretrained_layer = pretrained_model.middle_layers[i]\n",
    "    pinn.middle_layers[i].weight.data.copy_(pretrained_model.middle_layers[i].weight)\n",
    "    pinn.middle_layers[i].bias.data.copy_(pretrained_model.middle_layers[i].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train new network with pretrained weights and biases in middle layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qEPIGzEXj0b",
    "outputId": "4555c285-89a5-4e4f-ff57-1560c4f910b1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '2024-04-30' created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 6.8753e+06:   5%|‚ñç         | 2272/50000 [03:18<1:09:06, 11.51it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pinn_trained, loss_values \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpinn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EvolBERNPINN2D/pinn.py:298\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(nn_approximator, loss_fn, learning_rate, max_epochs)\u001b[0m\n\u001b[1;32m    295\u001b[0m _, residual_loss, initial_loss, boundary_loss \u001b[38;5;241m=\u001b[39m loss_fn\u001b[38;5;241m.\u001b[39mverbose(nn_approximator)\n\u001b[1;32m    296\u001b[0m loss: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m loss_fn(nn_approximator)\n\u001b[0;32m--> 298\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    301\u001b[0m loss_values\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.conda/envs/EvolBern/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/EvolBern/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/EvolBern/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pinn_trained, loss_values = train_model(\n",
    "    pinn, loss_fn=loss_fn, learning_rate=lr, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tp5m_1mJ0By3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_time = get_current_time(fmt=\"%H:%M\")\n",
    "\n",
    "folder = 'model'\n",
    "model_name = f'{lr}_{epochs}_{dim_hidden}_{current_time}.pth'\n",
    "model_path = os.path.join(folder, model_name)\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "torch.save(pinn_trained.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "yvHawzyAYkuW",
    "outputId": "4c3a7757-9298-4fe9-c1cb-1d159dc184ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from plots import plot_initial_conditions, plot_solution\n",
    "\n",
    "x, y, _ = get_initial_points(x_domain, y_domain, t_domain, n_train)\n",
    "t_value = 0.0\n",
    "t = torch.full_like(x, t_value)\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "t = t.to(device)\n",
    "z = f(pinn, x ,y, t)\n",
    "ux_0, uy_0 = initial_conditions(x, y, Lx, i = 1)\n",
    "z_0 = torch.cat((ux_0, uy_0), dim=1)\n",
    "\n",
    "plot_initial_conditions(z_0, y, x, 'Initial conditions - analytical', n_train, from_pinn = 0)\n",
    "plot_initial_conditions(z, y, x, 'Initial conditions - NN', n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "x, y, t = get_interior_points(x_domain, y_domain, t_domain, n_train)\n",
    "animation = plot_solution(pinn, x, y, t, n_train)\n",
    "HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hqnVSEf3dgC"
   },
   "source": [
    "# To be added\n",
    "- ~separate loss in more bars to see how the various loss term come to zero~\n",
    "- see if some quadrature rule has been implemented\n",
    "- scheme of weights initialization in order to automatically satisfy initial conditions\n",
    "- plots (in progress)\n",
    "- NN operators (to generalize results)\n",
    "- try to implement function that allows that satisfy initial conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLyts7jbh2i4"
   },
   "source": [
    "# To be fully understood\n",
    "- reshape when plotting (also different number of points allowed?)\n",
    "- ~not consistent initial conditions~ (between $u_x$ and $u_y$)?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "EvolBern:Python",
   "language": "python",
   "name": "conda-env-EvolBern-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
